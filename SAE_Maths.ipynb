{"cells":[{"cell_type":"markdown","metadata":{"id":"gcd_eIkFGNZR"},"source":["Importation des librairies et du dataset."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":27895,"status":"ok","timestamp":1701944287863,"user":{"displayName":"Romain Gouraud","userId":"02532941435331716181"},"user_tz":-60},"id":"kRDM3NBIZfeL"},"outputs":[],"source":["import json\n","import random\n","import spacy\n","from sklearn.svm import SVC\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report\n","import pandas as pd\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import cross_val_score\n","import numpy as np\n","from sklearn.metrics import confusion_matrix"]},{"cell_type":"markdown","metadata":{"id":"dz6gf67qHiyG"},"source":["# Partie 1 :Netoyage des données.\n","### Séparation des données et labélisation de celle-ci"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"mW40R9XrZxE2"},"outputs":[],"source":["rawData = []\n","with open(\"./data/image_review_all.json\") as fichier:\n","    for line in fichier:\n","        dataTempo = json.loads(line)\n","        rawData.append({\n","            \"id\": dataTempo.get('user_id') + \"_\" + dataTempo.get('business_id'),\n","            \"business_id\": dataTempo.get('business_id'),\n","            \"user_id\": dataTempo.get('user_id'),\n","            \"rating\": dataTempo.get('rating'),\n","            \"review_text\": dataTempo.get('review_text'),\n","            \"label\": 'positif' if dataTempo.get('rating') > 2 else 'negatif'\n","        })\n","rawDF = pd.DataFrame(rawData)"]},{"cell_type":"markdown","metadata":{"id":"SKevTeE1H02i"},"source":["### Séparation des données, pour un réaliser un apprentissage plus pertinent"]},{"cell_type":"markdown","metadata":{},"source":["En effet pour réaliser un apprentissage plus pertinent nous avons fait le choix d'avoir un dataset composé de moitier par des avis positifs et donc l'autre moitier par des avis négatifs"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":247},"executionInfo":{"elapsed":14,"status":"error","timestamp":1701944256467,"user":{"displayName":"Romain Gouraud","userId":"02532941435331716181"},"user_tz":-60},"id":"XJwXb3W0aGH4","outputId":"b03c386d-4ee5-42ed-dc2e-0e24b3cf8815"},"outputs":[{"name":"stdout","output_type":"stream","text":["Taille du DataFrame =  (193308, 6)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>business_id</th>\n","      <th>user_id</th>\n","      <th>rating</th>\n","      <th>review_text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1459413</th>\n","      <td>110934711920521036628_604088a37cd8bf130361dcf7</td>\n","      <td>604088a37cd8bf130361dcf7</td>\n","      <td>110934711920521036628</td>\n","      <td>1</td>\n","      <td>Why are you guys SO SLOW in the drive thru! Th...</td>\n","      <td>negatif</td>\n","    </tr>\n","    <tr>\n","      <th>1055668</th>\n","      <td>116012958129422282207_6046b8efb0e2129e4753558b</td>\n","      <td>6046b8efb0e2129e4753558b</td>\n","      <td>116012958129422282207</td>\n","      <td>5</td>\n","      <td>We found this place from Chet Garners show The...</td>\n","      <td>positif</td>\n","    </tr>\n","    <tr>\n","      <th>1408465</th>\n","      <td>108430402497176155594_604172957cd8bf1303625869</td>\n","      <td>604172957cd8bf1303625869</td>\n","      <td>108430402497176155594</td>\n","      <td>5</td>\n","      <td>Beautiful atmosphere, fantastic food, okay ser...</td>\n","      <td>positif</td>\n","    </tr>\n","    <tr>\n","      <th>394571</th>\n","      <td>101062954353697495551_60510eaada79151bfc125676</td>\n","      <td>60510eaada79151bfc125676</td>\n","      <td>101062954353697495551</td>\n","      <td>1</td>\n","      <td>Beef taco was delicious the chicken taco with ...</td>\n","      <td>negatif</td>\n","    </tr>\n","    <tr>\n","      <th>734176</th>\n","      <td>109035487472209572473_604b56a677e81aaed3cc93a2</td>\n","      <td>604b56a677e81aaed3cc93a2</td>\n","      <td>109035487472209572473</td>\n","      <td>1</td>\n","      <td>Worst McDonald's I have ever visited</td>\n","      <td>negatif</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                     id  \\\n","1459413  110934711920521036628_604088a37cd8bf130361dcf7   \n","1055668  116012958129422282207_6046b8efb0e2129e4753558b   \n","1408465  108430402497176155594_604172957cd8bf1303625869   \n","394571   101062954353697495551_60510eaada79151bfc125676   \n","734176   109035487472209572473_604b56a677e81aaed3cc93a2   \n","\n","                      business_id                user_id  rating  \\\n","1459413  604088a37cd8bf130361dcf7  110934711920521036628       1   \n","1055668  6046b8efb0e2129e4753558b  116012958129422282207       5   \n","1408465  604172957cd8bf1303625869  108430402497176155594       5   \n","394571   60510eaada79151bfc125676  101062954353697495551       1   \n","734176   604b56a677e81aaed3cc93a2  109035487472209572473       1   \n","\n","                                               review_text    label  \n","1459413  Why are you guys SO SLOW in the drive thru! Th...  negatif  \n","1055668  We found this place from Chet Garners show The...  positif  \n","1408465  Beautiful atmosphere, fantastic food, okay ser...  positif  \n","394571   Beef taco was delicious the chicken taco with ...  negatif  \n","734176                Worst McDonald's I have ever visited  negatif  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["positiveReviewDF = rawDF[rawDF['label'] == 'positif']\n","negativeReviewDF = rawDF[rawDF['label'] == 'negatif']\n","positiveReviewDF = positiveReviewDF.sample(frac=1, random_state=42)\n","\n","Data = pd.concat([negativeReviewDF, positiveReviewDF.head(min(len(negativeReviewDF), len(positiveReviewDF)))])\n","Data = Data.dropna()\n","Data = Data.sample(frac=1, random_state=42)\n","\n","print(\"Taille du DataFrame = \" ,Data.shape)\n","Data.head()"]},{"cell_type":"markdown","metadata":{"id":"iA2P8p-eeXPV"},"source":["Sélection de 5 000 données pour notre apprentissage, la tokenisation étant assez longue nous ne pouvons pas prendre le dataset entier."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"doZOOX-5d45I"},"outputs":[],"source":["DataForLearning = Data.head(5000)"]},{"cell_type":"markdown","metadata":{},"source":["Les données étant maintenant nétoyer nous allons pouvoir passer à la partie 2 : Mise en place de la classification avec un méthode vue en cours (deux \n","classes à prédire)"]},{"cell_type":"markdown","metadata":{},"source":["# Partie 2 : Mise en place de la classification avec un méthode vue en cours (deux classes à prédire)"]},{"cell_type":"markdown","metadata":{},"source":["Pour notre apprentissage nous utilisons spacy et plus particulièrement son module \"en_core_web_sm\" car nos avis sont exlusivement en anglais. Pour ce projet la version small du module est suffisant."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114170,"status":"ok","timestamp":1701780014884,"user":{"displayName":"Romain Gouraud","userId":"02532941435331716181"},"user_tz":-60},"id":"vfiXNXoSejAF","outputId":"809a5cff-208a-40e9-a64d-bf6f72cfa1d2"},"outputs":[{"data":{"text/plain":["1459413      guy slow drive ! bad location dining area close\n","1055668    find place Chet Garners daytripper . awesome p...\n","1408465    beautiful atmosphere , fantastic food , okay s...\n","394571     beef taco delicious chicken taco soft shell ho...\n","734176                                  Worst McDonald visit\n","                                 ...                        \n","266120     delicious paczki . order online easy . navigat...\n","620636     way price average food worth wait que .   orde...\n","435578     time , german restaurant matter . order pretze...\n","1429300    excellent CUISINE & AFFORDABLE & wide selectio...\n","1273015    savannah beer good , shrimp grit bad wife Ther...\n","Name: review_text, Length: 5000, dtype: object"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["nlp = spacy.load(\"en_core_web_sm\")\n","def tokenisation(text):\n","  return \" \".join([token.lemma_ for token in nlp(text) if not token.is_stop])\n","DataForLearning[\"review_text\"].apply(tokenisation)"]},{"cell_type":"markdown","metadata":{},"source":["### Première tentative pour la partie 2"]},{"cell_type":"markdown","metadata":{"id":"sT7jVeCWgPhg"},"source":["Nous vectorisons ensuite nos review textes pour cela nous utilisons le CountVectorizer celui-ci convertit des textes en vecteurs numériques en comptant les occurrences de chaque mot, créant ainsi une représentation matricielle. Puis nous utilisons trai_test_split comme vu en cours en effet celui-ci divise un ensemble de données en un ensemble d'entraînement et un ensemble de test, facilitant l'évaluation des performances des modèles d'apprentissage automatique en mesurant leur capacité à généraliser sur de nouvelles données. Nous utilisons cette technique pour tous nos classifier."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"cE-XXR0SgTId"},"outputs":[],"source":["vectorizer = CountVectorizer()\n","reviewVectorised = vectorizer.fit_transform(DataForLearning['review_text'])\n","trainX, testX, trainY, testY = train_test_split(reviewVectorised,DataForLearning[\"label\"] ,test_size=0.20)"]},{"cell_type":"markdown","metadata":{},"source":["Pour notre première tentative nous utilisions MultinomialNB en effet celui-ci est le plus approprié pour la classification de textes basée sur des comptages, comme la fréquence des mots."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1701784295304,"user":{"displayName":"Romain Gouraud","userId":"02532941435331716181"},"user_tz":-60},"id":"tFdQvbZWic3d","outputId":"8a16b027-e506-42e0-aa94-48aa491b0c27"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.878\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","     negatif       0.91      0.86      0.88       530\n","     positif       0.85      0.90      0.87       470\n","\n","    accuracy                           0.88      1000\n","   macro avg       0.88      0.88      0.88      1000\n","weighted avg       0.88      0.88      0.88      1000\n","\n"]}],"source":["classifier = MultinomialNB()\n","classifier.fit(trainX, trainY)\n","\n","# Make predictions on the test set\n","predictions = classifier.predict(testX)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(testY, predictions)\n","report = classification_report(testY, predictions)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Classification Report:\\n\", report)"]},{"cell_type":"markdown","metadata":{},"source":["Pour visualiser de façon plus concret nous avons fais le choix de tester avec des phrases qui ne sont PAS dans le dataset original pour tester la prédiction en condition réelle."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":775,"status":"ok","timestamp":1701780435351,"user":{"displayName":"Romain Gouraud","userId":"02532941435331716181"},"user_tz":-60},"id":"yvAXmUtSm9UZ","outputId":"96e720f6-96a4-4140-87e5-6b68eb677aad"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sentence: The service was slow, and the food was tasteless.\n","Predicted Sentiment: negatif\n","\n","Sentence: Amazing atmosphere and a diverse menu.\n","Predicted Sentiment: positif\n","\n","Sentence: I would not recommend this place. The service was terrible.\n","Predicted Sentiment: negatif\n","\n","Sentence: Delicious food and friendly staff.\n","Predicted Sentiment: positif\n","\n"]}],"source":["test_sentences = [\n","    \"The service was slow, and the food was tasteless.\",\n","    \"Amazing atmosphere and a diverse menu.\",\n","    \"I would not recommend this place. The service was terrible.\",\n","    \"Delicious food and friendly staff.\",\n","]\n","\n","\n","processed_test_sentences = [\n","    \" \".join([token.lemma_ for token in nlp(sentence) if not token.is_stop]) for sentence in test_sentences\n","]\n","\n","test_sentences_vectorized = vectorizer.transform(processed_test_sentences)\n","predictions_test = classifier.predict(test_sentences_vectorized)\n","\n","for sentence, prediction in zip(test_sentences, predictions_test):\n","    print(f\"Sentence: {sentence}\")\n","    print(f\"Predicted Sentiment: {prediction}\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["Les tests sont plus que concluant mais pour réaliser la partie bonus nous avons fais le choix de tester avec deux classifier différents"]},{"cell_type":"markdown","metadata":{"id":"hwUPP4WI9EQw"},"source":["### Alternative à la première tentative pour la partie 2"]},{"cell_type":"markdown","metadata":{},"source":["Pour cette première alternative nous avons fais le choix d'utiliser KNN (k plus proche voisin) pour appliquer un technique utiliser en cours."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":639,"status":"ok","timestamp":1701784303005,"user":{"displayName":"Romain Gouraud","userId":"02532941435331716181"},"user_tz":-60},"id":"LNYI2pHb-NE9","outputId":"095a96b4-b7dd-4e46-cfe3-468b1b05942d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (k-NN): 0.633\n","matrice confusion =  [[222 308]\n"," [ 59 411]]\n","Classification Report (k-NN):\n","               precision    recall  f1-score   support\n","\n","     negatif       0.79      0.42      0.55       530\n","     positif       0.57      0.87      0.69       470\n","\n","    accuracy                           0.63      1000\n","   macro avg       0.68      0.65      0.62      1000\n","weighted avg       0.69      0.63      0.62      1000\n","\n"]}],"source":["#Le code en commentaire nous a permit de sélectionner le meilleur k pour le KNN\n","\"\"\"\n","tab_score_binary = []\n","for k in range(1,50):\n","  kloop= KNeighborsClassifier(k)\n","  tab_score_binary.append(np.mean(cross_val_score(kloop, trainX, trainY, cv=10)))\n","knn = np.mean(tab_score_binary)\n","\"\"\"\n","knn =1\n","knn_classifier = KNeighborsClassifier(n_neighbors= knn )  # You can adjust the number of neighbors (n_neighbors) as needed\n","knn_classifier.fit(trainX, trainY)\n","\n","# Make predictions on the test set\n","predictions_knn = knn_classifier.predict(testX)\n","\n","# Evaluate the k-NN model\n","accuracy_knn = accuracy_score(testY, predictions_knn)\n","report_knn = classification_report(testY, predictions_knn)\n","\n","print(\"Accuracy (k-NN):\", accuracy_knn)\n","print(\"matrice confusion = \", confusion_matrix(testY, predictions_knn))\n","print(\"Classification Report (k-NN):\\n\", report_knn)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":235,"status":"ok","timestamp":1701784251167,"user":{"displayName":"Romain Gouraud","userId":"02532941435331716181"},"user_tz":-60},"id":"6hcVHTme_xsE","outputId":"97c1f075-27ad-4cd2-f3a5-6013296da5fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sentence: The service was slow, and the food was tasteless.\n","Predicted Sentiment: positif\n","\n","Sentence: Amazing atmosphere and a diverse menu.\n","Predicted Sentiment: positif\n","\n","Sentence: I would not recommend this place. The service was terrible.\n","Predicted Sentiment: positif\n","\n","Sentence: Delicious food and friendly staff.\n","Predicted Sentiment: positif\n","\n"]}],"source":["test_sentences = [\n","    \"The service was slow, and the food was tasteless.\",\n","    \"Amazing atmosphere and a diverse menu.\",\n","    \"I would not recommend this place. The service was terrible.\",\n","    \"Delicious food and friendly staff.\",\n","]\n","processed_test_sentences = [\n","    \" \".join([token.lemma_ for token in nlp(sentence) if not token.is_stop]) for sentence in test_sentences\n","]\n","\n","\n","test_sentences_vectorized = vectorizer.transform(processed_test_sentences)\n","\n","\n","predictions_test = knn_classifier.predict(test_sentences_vectorized)\n","\n","\n","for sentence, prediction in zip(test_sentences, predictions_test):\n","    print(f\"Sentence: {sentence}\")\n","    print(f\"Predicted Sentiment: {prediction}\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["Après analyse des résultats pour les mêmes phrases et le même jeu de données d'apprentissage le résultat est moins bon malgré un score d'accuracy d'environ 0.6-0.7"]},{"cell_type":"markdown","metadata":{"id":"xb8E7UYaAWN7"},"source":["### Alternative 3 pour la partie 2 : Utilisation de SVC"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4802,"status":"ok","timestamp":1701784334044,"user":{"displayName":"Romain Gouraud","userId":"02532941435331716181"},"user_tz":-60},"id":"p3y8YSAVAVd_","outputId":"82c4643a-9505-4141-adbf-6418da783f23"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (SVM): 0.881\n","Classification Report (SVM):\n","               precision    recall  f1-score   support\n","\n","     negatif       0.90      0.87      0.88       523\n","     positif       0.86      0.90      0.88       477\n","\n","    accuracy                           0.88      1000\n","   macro avg       0.88      0.88      0.88      1000\n","weighted avg       0.88      0.88      0.88      1000\n","\n"]}],"source":["vectorizer = CountVectorizer()\n","reviewVectorised = vectorizer.fit_transform(DataForLearning['review_text'])\n","trainX, testX, trainY, testY = train_test_split(reviewVectorised,DataForLearning[\"label\"] ,test_size=0.20)\n","\n","svm_classifier = SVC(kernel='linear') \n","svm_classifier.fit(trainX, trainY)\n","\n","predictions_svm = svm_classifier.predict(testX)\n","accuracy_svm = accuracy_score(testY, predictions_svm)\n","report_svm = classification_report(testY, predictions_svm)\n","\n","print(\"Accuracy (SVM):\", accuracy_svm)\n","print(\"Classification Report (SVM):\\n\", report_svm)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":240,"status":"ok","timestamp":1701783037274,"user":{"displayName":"Romain Gouraud","userId":"02532941435331716181"},"user_tz":-60},"id":"4_m-_cCOK-Ei","outputId":"d3994986-451e-4391-f0bb-e030643ee45e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sentence: The service was slow, and the food was tasteless.\n","Predicted Sentiment: negatif\n","\n","Sentence: Amazing atmosphere and a diverse menu.\n","Predicted Sentiment: positif\n","\n","Sentence: I would not recommend this place. The service was terrible.\n","Predicted Sentiment: negatif\n","\n","Sentence: Delicious food and friendly staff.\n","Predicted Sentiment: positif\n","\n"]}],"source":["test_sentences = [\n","    \"The service was slow, and the food was tasteless.\",\n","    \"Amazing atmosphere and a diverse menu.\",\n","    \"I would not recommend this place. The service was terrible.\",\n","    \"Delicious food and friendly staff.\",\n","]\n","\n","\n","processed_test_sentences = [\n","    \" \".join([token.lemma_ for token in nlp(sentence) if not token.is_stop]) for sentence in test_sentences\n","]\n","\n","\n","test_sentences_vectorized = vectorizer.transform(processed_test_sentences)\n","\n","\n","predictions_test = svm_classifier.predict(test_sentences_vectorized)\n","\n","\n","for sentence, prediction in zip(test_sentences, predictions_test):\n","    print(f\"Sentence: {sentence}\")\n","    print(f\"Predicted Sentiment: {prediction}\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["Après analyse des résultats nous constatons un résultat plus que concluant mais il reste moins bon que le cas d'utilisation de MultinomialNB."]},{"cell_type":"markdown","metadata":{"id":"PIQjx-7UMU23"},"source":["# Partie 3"]},{"cell_type":"markdown","metadata":{"id":"gx0Wh-5HMYDh"},"source":["Pour cette partie aussi nous allons utiliser trois classifier pour répondre au bonnus. \\\n","\n","### Première cas utilisation de MultinomialNB."]},{"cell_type":"code","execution_count":25,"metadata":{"id":"W6s0Gm0yMZig"},"outputs":[],"source":["trainX, testX, trainY, testY = train_test_split(reviewVectorised,DataForLearning[\"rating\"] ,test_size=0.20)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":646,"status":"ok","timestamp":1701784719786,"user":{"displayName":"Romain Gouraud","userId":"02532941435331716181"},"user_tz":-60},"id":"NnLK_JJNONSe","outputId":"383bd7de-e886-4a4d-ea83-b8a13e306760"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.636\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           1       0.66      0.83      0.73       317\n","           2       0.41      0.38      0.39       202\n","           3       0.00      0.00      0.00        35\n","           4       0.16      0.06      0.09       100\n","           5       0.78      0.84      0.81       346\n","\n","    accuracy                           0.64      1000\n","   macro avg       0.40      0.42      0.40      1000\n","weighted avg       0.58      0.64      0.60      1000\n","\n"]}],"source":["vectorizer = CountVectorizer()\n","reviewVectorised = vectorizer.fit_transform(DataForLearning['review_text'])\n","trainX, testX, trainY, testY = train_test_split(reviewVectorised,DataForLearning[\"rating\"] ,test_size=0.20)\n","classifier = MultinomialNB()\n","classifier.fit(trainX, trainY)\n","\n","\n","predictions = classifier.predict(testX)\n","\n","\n","accuracy = accuracy_score(testY, predictions)\n","report = classification_report(testY, predictions)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Classification Report:\\n\", report)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1701784738039,"user":{"displayName":"Romain Gouraud","userId":"02532941435331716181"},"user_tz":-60},"id":"ehybxmhUOToH","outputId":"ec29ee2e-2d20-4a39-864b-5b3e88864a50"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sentence: The service was slow, and the food was tasteless.\n","Predicted Sentiment: 1\n","\n","Sentence: Amazing atmosphere and a diverse menu.\n","Predicted Sentiment: 5\n","\n","Sentence: I would not recommend this place. The service was terrible.\n","Predicted Sentiment: 1\n","\n","Sentence: Delicious food and friendly staff.\n","Predicted Sentiment: 5\n","\n"]}],"source":["test_sentences = [\n","    \"The service was slow, and the food was tasteless.\",\n","    \"Amazing atmosphere and a diverse menu.\",\n","    \"I would not recommend this place. The service was terrible.\",\n","    \"Delicious food and friendly staff.\",\n","]\n","\n","\n","processed_test_sentences = [\n","    \" \".join([token.lemma_ for token in nlp(sentence) if not token.is_stop]) for sentence in test_sentences\n","]\n","\n","test_sentences_vectorized = vectorizer.transform(processed_test_sentences)\n","predictions_test = classifier.predict(test_sentences_vectorized)\n","\n","\n","for sentence, prediction in zip(test_sentences, predictions_test):\n","    print(f\"Sentence: {sentence}\")\n","    print(f\"Predicted Sentiment: {prediction}\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["Les tests pour ce classifier sont concluant cependant comme prévu la prediction pour notes dites neutres (3) est quasiment impossible."]},{"cell_type":"markdown","metadata":{"id":"7R2WQLHpQYBx"},"source":["### Alternative 2 à la partie 3 : Utilisation de la méthode KNN:"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1127,"status":"ok","timestamp":1701784755008,"user":{"displayName":"Romain Gouraud","userId":"02532941435331716181"},"user_tz":-60},"id":"1dwEn-bFQfod","outputId":"f5734b18-b996-42f2-eaa6-d6b66808e141"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (k-NN): 0.435\n","matrice confusion =  [[103  64  13  23 109]\n"," [ 59  44  11  36  69]\n"," [  0   3   2   7  13]\n"," [  6   6   2  37  52]\n"," [ 15  11   2  64 249]]\n","Classification Report (k-NN):\n","               precision    recall  f1-score   support\n","\n","           1       0.56      0.33      0.42       312\n","           2       0.34      0.20      0.25       219\n","           3       0.07      0.08      0.07        25\n","           4       0.22      0.36      0.27       103\n","           5       0.51      0.73      0.60       341\n","\n","    accuracy                           0.43      1000\n","   macro avg       0.34      0.34      0.32      1000\n","weighted avg       0.45      0.43      0.42      1000\n","\n"]}],"source":["vectorizer = CountVectorizer()\n","reviewVectorised = vectorizer.fit_transform(DataForLearning['review_text'])\n","trainX, testX, trainY, testY = train_test_split(reviewVectorised,DataForLearning[\"rating\"] ,test_size=0.20)\n","\n","knn =1\n","knn_classifier = KNeighborsClassifier(n_neighbors= knn )  \n","knn_classifier.fit(trainX, trainY)\n","\n","\n","predictions_knn = knn_classifier.predict(testX)\n","\n","\n","accuracy_knn = accuracy_score(testY, predictions_knn)\n","report_knn = classification_report(testY, predictions_knn)\n","\n","print(\"Accuracy (k-NN):\", accuracy_knn)\n","print(\"matrice confusion = \", confusion_matrix(testY, predictions_knn))\n","print(\"Classification Report (k-NN):\\n\", report_knn)"]},{"cell_type":"markdown","metadata":{},"source":["Comme lors de la partie 2 ce classifier ne répond pas aux attentes fixé pour l'exercice, avec une prédiction plus que peux fiable. Suite à ce manque de résultat concluant nous ne montrerons pas d'exemple."]},{"cell_type":"markdown","metadata":{"id":"WGTN-HdqRrdP"},"source":["### Alternative 3 pour la partie 3 : Utilisation de SVC Linear"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8726,"status":"ok","timestamp":1701786290698,"user":{"displayName":"Romain Gouraud","userId":"02532941435331716181"},"user_tz":-60},"id":"8Mfo9GYjRya9","outputId":"ffcef36e-4926-4b0e-da1f-67c0b5328c89"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (SVM): 0.604\n","Classification Report (SVM):\n","               precision    recall  f1-score   support\n","\n","           1       0.62      0.66      0.64       314\n","           2       0.39      0.37      0.38       201\n","           3       0.04      0.05      0.04        19\n","           4       0.34      0.29      0.32       102\n","           5       0.81      0.80      0.80       364\n","\n","    accuracy                           0.60      1000\n","   macro avg       0.44      0.44      0.44      1000\n","weighted avg       0.60      0.60      0.60      1000\n","\n"]}],"source":["vectorizer = CountVectorizer()\n","reviewVectorised = vectorizer.fit_transform(DataForLearning['review_text'])\n","trainX, testX, trainY, testY = train_test_split(reviewVectorised,DataForLearning[\"rating\"] ,test_size=0.20)\n","\n","svm_classifier = SVC(kernel='linear')\n","svm_classifier.fit(trainX, trainY)\n","\n","\n","predictions_svm = svm_classifier.predict(testX)\n","\n","\n","accuracy_svm = accuracy_score(testY, predictions_svm)\n","report_svm = classification_report(testY, predictions_svm)\n","\n","print(\"Accuracy (SVM):\", accuracy_svm)\n","print(\"Classification Report (SVM):\\n\", report_svm)"]},{"cell_type":"markdown","metadata":{},"source":["Après analyse des résultats nous pouvons conclure que ce classifier est peut fiable mais reste notre second plus fiable avec en moyenne 3% de score d'accuracy de différence avec MultinomialNB."]},{"cell_type":"markdown","metadata":{},"source":["# Partie 4 : Bonus"]},{"cell_type":"markdown","metadata":{},"source":["Cette partie à été raliser tout au long des parties 2 et 3. Via l'utilation de différent classifier."]},{"cell_type":"markdown","metadata":{},"source":["A partir de la découmentation j'ai tenté de résumer au mieu comment fonctionne les différents classifier: \\\n"," KNN fonctionne en trouvant les voisins les plus proches dans l'espace des caractéristiques pour prédire la classe d'un point, adapté à des données de dimensions moyennes à élevées. \\\n"," SVC cherche un hyperplan optimal de séparation entre les classes, adapté aux problèmes de classification linéaire et non linéaire. \\\n"," MultinomialNB, quant à lui, repose sur le théorème de Bayes et est particulièrement utile pour les données textuelles, modélisant la distribution multinomiale des caractéristiques"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
